import tensorflow as tf
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from collections import deque
import random
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.layers import Lambda
from tensorflow.keras.losses import Huber


class TradingEnvironment:
    def __init__(self, data, window_size=10):
        self.data = data
        self.window_size = window_size
        self.state_size = window_size * 5
        self.action_size = 3
        self.scaler = StandardScaler()
        self._prepare_data()
        self.reset()

    def _prepare_data(self):
        self.scaled_data = self.scaler.fit_transform(self.data)
        self.total_steps = len(self.data) - self.window_size

    def get_state(self):
        window = self.scaled_data[self.current_step:self.current_step + self.window_size]
        return window.flatten()

    def reset(self):
        self.current_step = 0
        self.cash = 10000
        self.shares = 0
        self.total_value_history = [self.cash]
        self.trades = []
        return self.get_state()

    def step(self, action):
        current_price = self.data.iloc[self.current_step + self.window_size]['Close']

        reward = 0
        if action == 0:
            if self.cash > current_price:
                shares_to_buy = self.cash // current_price
                self.shares += shares_to_buy
                self.cash -= shares_to_buy * current_price
                self.trades.append(('buy', self.current_step + self.window_size, current_price))
                reward = 0.1
        elif action == 1:
            if self.shares > 0:
                self.cash += self.shares * current_price
                self.trades.append(('sell', self.current_step + self.window_size, current_price))
                reward = 0.1
                self.shares = 0

        self.current_step += 1

        total_value = self.cash + self.shares * current_price
        self.total_value_history.append(total_value)

        if len(self.total_value_history) > 1:
            value_change = (total_value - self.total_value_history[-2]) / self.total_value_history[-2]
            reward += value_change * 100

        done = self.current_step >= self.total_steps
        next_state = self.get_state() if not done else None

        return next_state, reward, done


class AdvancedDQNAgent:
    def __init__(self, state_size, action_size):
        self.state_size = state_size
        self.action_size = action_size
        self.memory = deque(maxlen=10000)
        self.gamma = 0.95
        self.epsilon = 1.0
        self.epsilon_min = 0.01
        self.epsilon_decay = 0.995
        self.learning_rate = 0.001
        self.model = self._build_model()
        self.target_model = self._build_model()
        self.update_target_counter = 0

    def _build_model(self):
        inputs = tf.keras.Input(shape=(self.state_size,))

        x = tf.keras.layers.Dense(64, activation='relu')(inputs)
        x = tf.keras.layers.BatchNormalization()(x)
        x = tf.keras.layers.Dense(32, activation='relu')(x)
        x = tf.keras.layers.BatchNormalization()(x)

        value = tf.keras.layers.Dense(16, activation='relu')(x)
        value = tf.keras.layers.Dense(1)(value)

        advantage = tf.keras.layers.Dense(16, activation='relu')(x)
        advantage = tf.keras.layers.Dense(self.action_size)(advantage)

        def calculate_q_value(inputs):
            value, advantage = inputs
            return value + (advantage - tf.reduce_mean(advantage, axis=1, keepdims=True))

        outputs = Lambda(calculate_q_value)([value, advantage])

        model = tf.keras.Model(inputs=inputs, outputs=outputs)

        model.compile(loss=Huber(), optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate))
        return model

    def remember(self, state, action, reward, next_state, done):
        self.memory.append((state, action, reward, next_state, done))

    def act(self, state, training=True):
        if training and np.random.rand() <= self.epsilon:
            return random.randrange(self.action_size)
        
        act_values = self.model.predict(state.reshape(1, -1), verbose=0)
        
        return np.argmax(act_values[0])

    def replay(self, batch_size):
        if len(self.memory) < batch_size:
            return

        minibatch = random.sample(self.memory, batch_size)
        states = np.array([i[0] for i in minibatch])
        actions = np.array([i[1] for i in minibatch])
        rewards = np.array([i[2] for i in minibatch])
        next_states = np.array([i[3] for i in minibatch])
        dones = np.array([i[4] for i in minibatch])

        targets = self.model.predict(states, verbose=0)
        target_next = self.target_model.predict(next_states, verbose=0)

        for i in range(batch_size):
            if dones[i]:
                targets[i][actions[i]] = rewards[i]
            else:
                targets[i][actions[i]] = rewards[i] + self.gamma * np.max(target_next[i])

        self.model.fit(states, targets, epochs=1, verbose=0)

        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay

    def update_target_model(self):
        self.target_model.set_weights(self.model.get_weights())


def create_sample_data(n_points=1000):
    np.random.seed(42)
    dates = pd.date_range(start='2020-01-01', periods=n_points)

    base_price = 100
    trend = np.linspace(0, 20, n_points)
    noise = np.random.normal(0, 1, n_points)
    price = base_price + trend + noise.cumsum()

    data = pd.DataFrame({
        'Open': price + np.random.normal(0, 0.5, n_points),
        'High': price + np.random.normal(1, 0.5, n_points),
        'Low': price - np.random.normal(1, 0.5, n_points),
        'Close': price + np.random.normal(0, 0.5, n_points),
        'Volume': np.random.normal(1000000, 200000, n_points)
    }, index=dates)

    return data


def visualize_training(env, episode_rewards, total_value_history):
    plt.figure(figsize=(15, 10))

    plt.subplot(2, 1, 1)
    plt.plot(episode_rewards)
    plt.title('Episode Rewards Over Time')
    plt.xlabel('Episode')
    plt.ylabel('Total Reward')

    plt.subplot(2, 1, 2)
    plt.plot(total_value_history)
    plt.title('Portfolio Value Over Time')
    plt.xlabel('Step')
    plt.ylabel('Total Value ($)')

    for trade in env.trades:
        action, step, price = trade
        color = 'g' if action == 'buy' else 'r'
        plt.plot(step, price, marker='o', color=color, markersize=10)

    plt.tight_layout()
    plt.show()


def train_trading_agent(episodes=100):
    data = create_sample_data()
    env = TradingEnvironment(data)
    agent = AdvancedDQNAgent(env.state_size, env.action_size)

    batch_size = 32
    episode_rewards = []

    for episode in range(episodes):
        state = env.reset()
        total_reward = 0

        for time in range(env.total_steps):
            action = agent.act(state)
            next_state, reward, done = env.step(action)
            total_reward += reward

            if next_state is not None:
                agent.remember(state, action, reward, next_state, done)
                state = next_state

            if len(agent.memory) > batch_size:
                agent.replay(batch_size)

            if done:
                break

        if episode % 10 == 0:
            agent.update_target_model()

        episode_rewards.append(total_reward)
        print(f"Episode: {episode + 1}/{episodes}, Total Reward: {total_reward:.2f}, "
              f"Final Portfolio Value: ${env.total_value_history[-1]:.2f}, "
              f"Epsilon: {agent.epsilon:.2f}")

        if (episode + 1) % 20 == 0:
            visualize_training(env, episode_rewards, env.total_value_history)

    return agent, env, episode_rewards


if __name__ == "__main__":
    trained_agent, final_env, rewards = train_trading_agent()
    
