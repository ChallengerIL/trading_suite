import tensorflow as tf
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from collections import deque
import random
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization, Conv1D, MaxPooling1D, Flatten
from tensorflow.keras.regularizers import l2

# Switch to actual market data
# Adjust the indicators combination, ensure there's no lookahead bias while calculating the indicators
# Split the data into training and test batches
# Enhance the model structure


class EnhancedTradingEnvironment:
    def __init__(self, data, window_size=30, commission=0.001, spread=0.0002):
        self.data = data
        self.window_size = window_size
        self.commission = commission
        self.spread = spread

        self.base_features = ['Open', 'High', 'Low', 'Close', 'Volume']
        self.tech_features = ['SMA_10', 'SMA_30', 'RSI', 'MACD',
                              'BB_middle', 'BB_upper', 'BB_lower', 'Volume_SMA']
        self.all_features = self.base_features + self.tech_features

        self.n_features = len(self.all_features)
        self.state_size = self.window_size * self.n_features
        self.action_size = 3
        self.scaler = StandardScaler()
        self._prepare_data()
        self.reset()

    def _prepare_data(self):
        enhanced_data = self._calculate_technical_indicators()
        self.scaled_data = self.scaler.fit_transform(enhanced_data[self.all_features])
        self.total_steps = len(self.data) - self.window_size

    def get_state(self):
        window = self.scaled_data[self.current_step:self.current_step + self.window_size]
        return window.flatten()

    def _calculate_technical_indicators(self):
        df = self.data.copy()
        df['SMA_10'] = df['Close'].rolling(window=10).mean()
        df['SMA_30'] = df['Close'].rolling(window=30).mean()

        delta = df['Close'].diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
        rs = gain / loss
        df['RSI'] = 100 - (100 / (1 + rs))

        exp1 = df['Close'].ewm(span=12, adjust=False).mean()
        exp2 = df['Close'].ewm(span=26, adjust=False).mean()
        df['MACD'] = exp1 - exp2

        df['BB_middle'] = df['Close'].rolling(window=20).mean()
        df['BB_upper'] = df['BB_middle'] + 2 * df['Close'].rolling(window=20).std()
        df['BB_lower'] = df['BB_middle'] - 2 * df['Close'].rolling(window=20).std()

        df['Volume_SMA'] = df['Volume'].rolling(window=20).mean()
        df = df.bfill()
        return df

    def _calculate_transaction_price(self, base_price, is_buy):
        if is_buy:
            return base_price * (1 + self.spread / 2)
        return base_price * (1 - self.spread / 2)

    def reset(self):
        self.current_step = 0
        self.cash = 10000
        self.shares = 0
        self.total_value_history = [self.cash]
        self.trades = []
        self.positions = []
        self.current_position_start = None
        self.peak_value = self.cash
        return self.get_state()

    def step(self, action):
        current_price = self.data.iloc[self.current_step + self.window_size]['Close']
        reward = 0
        position_reward = 0
        trading_cost = 0

        if action == 0:
            if self.cash > current_price:
                transaction_price = self._calculate_transaction_price(current_price, True)
                max_shares = self.cash // transaction_price
                shares_to_buy = max_shares // 2

                commission_cost = shares_to_buy * transaction_price * self.commission
                total_cost = (shares_to_buy * transaction_price) + commission_cost

                if total_cost <= self.cash:
                    self.shares += shares_to_buy
                    self.cash -= total_cost
                    self.trades.append(('buy', self.current_step + self.window_size, transaction_price))
                    trading_cost -= commission_cost
                    self.current_position_start = self.current_step

        elif action == 1:
            if self.shares > 0:
                transaction_price = self._calculate_transaction_price(current_price, False)
                commission_cost = self.shares * transaction_price * self.commission
                total_revenue = (self.shares * transaction_price) - commission_cost

                self.cash += total_revenue
                trading_cost -= commission_cost
                self.trades.append(('sell', self.current_step + self.window_size, transaction_price))

                if self.current_position_start is not None:
                    position_duration = self.current_step - self.current_position_start
                    self.positions.append(position_duration)
                    position_reward = 0.1 * np.log(position_duration + 1)

                self.shares = 0
                self.current_position_start = None

        self.current_step += 1

        total_value = self.cash + self.shares * current_price
        self.total_value_history.append(total_value)

        if total_value > self.peak_value:
            self.peak_value = total_value

        drawdown = (self.peak_value - total_value) / self.peak_value if self.peak_value > 0 else 0

        if len(self.total_value_history) > 1:
            returns = (total_value - self.total_value_history[-2]) / self.total_value_history[-2]
            reward = (returns * 100)
            reward += position_reward
            reward += trading_cost
            reward -= 10 * drawdown

            if len(self.trades) > 2:
                recent_trades = self.trades[-3:]
                if len(set([t[0] for t in recent_trades])) == 1:
                    reward -= 0.5

        done = self.current_step >= self.total_steps
        next_state = self.get_state() if not done else None

        return next_state, reward, done


class AdvancedDQNAgent:
    def __init__(self, state_size, action_size):
        self.state_size = state_size
        self.action_size = action_size
        self.memory = deque(maxlen=20000)
        self.gamma = 0.95
        self.epsilon = 1.0
        self.epsilon_min = 0.01
        self.epsilon_decay = 0.995
        self.learning_rate = 0.0001
        self.n_features = 13
        self.model = self._build_model()
        self.target_model = self._build_model()
        self.batch_size = 64
        self.min_replay_memory = 1000

    def _build_model(self):
        sequence_length = self.state_size // self.n_features

        inputs = tf.keras.Input(shape=(self.state_size,))
        reshaped = tf.keras.layers.Reshape((sequence_length, self.n_features))(inputs)

        x = Conv1D(64, 3, activation='relu', kernel_regularizer=l2(0.01), padding='same')(reshaped)
        x = BatchNormalization()(x)
        x = MaxPooling1D(2)(x)
        x = Dropout(0.2)(x)

        x = Conv1D(128, 3, activation='relu', kernel_regularizer=l2(0.01), padding='same')(x)
        x = BatchNormalization()(x)
        x = MaxPooling1D(2)(x)
        x = Dropout(0.2)(x)

        x = LSTM(64, return_sequences=True)(x)
        x = Dropout(0.2)(x)
        x = LSTM(32)(x)
        x = BatchNormalization()(x)

        x = Dense(32, activation='relu', kernel_regularizer=l2(0.01))(x)
        x = BatchNormalization()(x)
        x = Dropout(0.2)(x)

        value_stream = Dense(16, activation='relu')(x)
        value = Dense(1)(value_stream)

        advantage_stream = Dense(16, activation='relu')(x)
        advantage = Dense(self.action_size)(advantage_stream)

        outputs = tf.keras.layers.Add()([
            value,
            tf.keras.layers.Subtract()([
                advantage,
                tf.keras.layers.Lambda(lambda x: tf.reduce_mean(x, axis=1, keepdims=True))(advantage)
            ])
        ])

        model = tf.keras.Model(inputs=inputs, outputs=outputs)
        model.compile(
            optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate),
            loss=tf.keras.losses.Huber()
        )

        return model

    def replay(self):
        if len(self.memory) < self.min_replay_memory:
            return

        minibatch = random.sample(self.memory, self.batch_size)

        states = np.array([i[0] for i in minibatch])
        actions = np.array([i[1] for i in minibatch])
        rewards = np.array([i[2] for i in minibatch])
        next_states = np.array([i[3] for i in minibatch])
        dones = np.array([i[4] for i in minibatch])

        assert states.shape[1] == self.state_size, f"State shape mismatch: {states.shape[1]} vs {self.state_size}"

        current_q_values = self.model.predict(states, verbose=0)
        next_q_values = self.model.predict(next_states, verbose=0)
        target_q_values = self.target_model.predict(next_states, verbose=0)

        for i in range(self.batch_size):
            if dones[i]:
                current_q_values[i][actions[i]] = rewards[i]
            else:
                best_action = np.argmax(next_q_values[i])
                current_q_values[i][actions[i]] = rewards[i] + self.gamma * target_q_values[i][best_action]

        self.model.fit(states, current_q_values, epochs=1, verbose=0, batch_size=self.batch_size)

        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay

    def remember(self, state, action, reward, next_state, done):
        self.memory.append((state, action, reward, next_state, done))

    def act(self, state, training=True):
        if training and np.random.rand() <= self.epsilon:
            return random.randrange(self.action_size)

        act_values = self.model.predict(state.reshape(1, -1), verbose=0)
        return np.argmax(act_values[0])

    def update_target_model(self):
        tau = 0.001
        weights = self.model.get_weights()
        target_weights = self.target_model.get_weights()

        for i in range(len(weights)):
            target_weights[i] = tau * weights[i] + (1 - tau) * target_weights[i]

        self.target_model.set_weights(target_weights)


def create_realistic_market_data(n_points=1000, volatility=0.02):
    np.random.seed(42)
    dates = pd.date_range(start='2020-01-01', periods=n_points)

    returns = np.random.normal(0.0002, volatility, n_points)
    price = 100 * np.exp(np.cumsum(returns))

    trend = np.linspace(0, 20, n_points)
    seasonal = 5 * np.sin(np.linspace(0, 8 * np.pi, n_points))
    price = price + trend + seasonal

    data = pd.DataFrame({
        'Open': price * (1 + np.random.normal(0, 0.001, n_points)),
        'High': price * (1 + np.abs(np.random.normal(0, 0.002, n_points))),
        'Low': price * (1 - np.abs(np.random.normal(0, 0.002, n_points))),
        'Close': price * (1 + np.random.normal(0, 0.001, n_points)),
        'Volume': np.random.gamma(2, 100000, n_points) * (1 + np.abs(returns))
    }, index=dates)

    data['High'] = data[['Open', 'High', 'Close']].max(axis=1)
    data['Low'] = data[['Open', 'Low', 'Close']].min(axis=1)

    return data


def visualize_training(env, episode_rewards, total_value_history, episode):
    plt.figure(figsize=(15, 12))

    plt.subplot(3, 1, 1)
    plt.plot(episode_rewards)
    plt.title(f'Episode Rewards Over Time (Episode {episode})')
    plt.xlabel('Episode')
    plt.ylabel('Total Reward')

    plt.subplot(3, 1, 2)
    plt.plot(total_value_history)
    plt.title('Portfolio Value Over Time')
    plt.xlabel('Step')
    plt.ylabel('Total Value ($)')

    plt.subplot(3, 1, 3)
    plt.plot(env.data.index[env.window_size:], env.data['Close'][env.window_size:], label='Close Price')
    plt.title('Trading Actions')
    plt.xlabel('Date')
    plt.ylabel('Price')

    for trade in env.trades:
        action, step, price = trade
        color = 'g' if action == 'buy' else 'r'
        marker = '^' if action == 'buy' else 'v'
        plt.plot(env.data.index[step], price, marker=marker, color=color, markersize=10)

    plt.legend()
    plt.tight_layout()
    plt.show()


def train_trading_agent(episodes=200, evaluation_interval=10):
    data = create_realistic_market_data(n_points=1000)

    env = EnhancedTradingEnvironment(data)
    agent = AdvancedDQNAgent(env.state_size, env.action_size)

    episode_rewards = []
    best_reward = float('-inf')
    reward_window = deque(maxlen=10)

    for episode in range(episodes):
        state = env.reset()
        total_reward = 0
        done = False

        while not done:
            action = agent.act(state)

            next_state, reward, done = env.step(action)
            total_reward += reward

            if next_state is not None:
                agent.remember(state, action, reward, next_state, done)
                state = next_state

            if len(agent.memory) >= agent.min_replay_memory:
                agent.replay()

        if episode % 5 == 0:
            agent.update_target_model()

        episode_rewards.append(total_reward)
        reward_window.append(total_reward)
        avg_reward = np.mean(reward_window)

        if avg_reward > best_reward:
            best_reward = avg_reward
            agent.model.save('best_trading_model.keras')

        print(f"Episode: {episode + 1}/{episodes}")
        print(f"Total Reward: {total_reward:.2f}")
        print(f"Final Portfolio Value: ${env.total_value_history[-1]:.2f}")
        print(f"Epsilon: {agent.epsilon:.3f}")
        print(f"Average Reward (last 10): {avg_reward:.2f}")
        print("----------------------------------------")

        if (episode + 1) % evaluation_interval == 0:
            visualize_training(env, episode_rewards, env.total_value_history, episode + 1)

    return agent, env, episode_rewards


def evaluate_agent(agent, data, episodes=5):
    env = EnhancedTradingEnvironment(data)
    evaluation_results = []

    for episode in range(episodes):
        state = env.reset()
        done = False

        while not done:
            action = agent.act(state, training=False)
            next_state, reward, done = env.step(action)
            state = next_state if not done else None

        final_value = env.total_value_history[-1]
        returns = (final_value - 10000) / 10000 * 100
        num_trades = len(env.trades)

        result = {
            'episode': episode + 1,
            'final_value': final_value,
            'returns': returns,
            'num_trades': num_trades
        }
        evaluation_results.append(result)

        print(f"Evaluation Episode {episode + 1}:")
        print(f"Final Portfolio Value: ${final_value:.2f}")
        print(f"Return: {returns:.2f}%")
        print(f"Number of Trades: {num_trades}")
        print("----------------------------------------")

    return evaluation_results


if __name__ == "__main__":
    trained_agent, final_env, rewards = train_trading_agent()
    test_data = create_realistic_market_data(n_points=500)
    evaluation_results = evaluate_agent(trained_agent, test_data)

    returns = [result['returns'] for result in evaluation_results]
    trades = [result['num_trades'] for result in evaluation_results]

    print("\nEvaluation Summary:")
    print(f"Average Return: {np.mean(returns):.2f}%")
    print(f"Return Std Dev: {np.std(returns):.2f}%")
    print(f"Average Trades per Episode: {np.mean(trades):.1f}")
    print(f"Best Return: {max(returns):.2f}%")
    print(f"Worst Return: {min(returns):.2f}%")
